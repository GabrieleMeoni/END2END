{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing results notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting devices..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"..\")\n",
    "from datasets.ssl_dataset import SSL_Dataset\n",
    "from datasets.data_utils import get_data_loader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from glob import glob\n",
    "from utils import net_builder\n",
    "from train_utils import mcc\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir=\"/home/gabrielemeoni/project/END2END/MSMatch/data/\"\n",
    "test_dset = SSL_Dataset(name=\"thraws_swir_test\", train=False, data_dir=dataset_dir)\n",
    "\n",
    "test_dset_basic = test_dset.get_dset()\n",
    "num_classes = test_dset.num_classes\n",
    "num_channels = test_dset.num_channels\n",
    "eval_loader = get_data_loader(test_dset_basic, 8, num_workers=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint path..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path=\"/home/gabrielemeoni/project/END2END/MSMatch/checkpoints/paper_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing tests...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_checkpoint_seed_paths=sorted(glob(os.path.join(checkpoint_path, \"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=[]\n",
    "test_results_mcc_seed=[]\n",
    "test_results_acc_seed=[]\n",
    "\n",
    "for test_seed_path in tqdm(test_checkpoint_seed_paths, desc=\"Processing seed...\"):\n",
    "    seeds.append(int(test_seed_path.split(os.sep)[-1].split(\"_\")[-1]))\n",
    "    upsampling_values_test_paths=sorted(glob(os.path.join(test_seed_path, \"*\")))\n",
    "    test_results_acc_seed_upsample=[]\n",
    "    test_results_mcc_seed_upsample=[]\n",
    "    upsampling_values=[]\n",
    "    for test in upsampling_values_test_paths:\n",
    "        upsampling_values.append(int(test.split(\"_{\")[1].split(\"}\")[0]))\n",
    "\n",
    "        #Exploring the whole path until you reach the final directory\n",
    "        while(len(glob(os.path.join(test, \"*\"))) == 1):\n",
    "            test=os.path.join(test, glob(os.path.join(test, \"*\"))[0])\n",
    "            \n",
    "        checkpoint_path = os.path.join(test, \"model_best.pth\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        load_model = (checkpoint[\"eval_model\"])\n",
    "\n",
    "        _net_builder = net_builder(\n",
    "            \"efficientnet-lite0\",\n",
    "            False,\n",
    "            {\n",
    "                \"depth\": 28,\n",
    "                \"widen_factor\": 2,\n",
    "                \"leaky_slope\": 0.1,\n",
    "                \"dropRate\": 0.0,\n",
    "            },\n",
    "        )\n",
    "        net = _net_builder(num_classes=num_classes, in_channels=num_channels)\n",
    "        net.load_state_dict(load_model)\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "        net.eval()\n",
    "        \n",
    "        acc = 0.0\n",
    "        y_true=[]\n",
    "        y_pred=[]\n",
    "        n=0\n",
    "        with torch.no_grad():\n",
    "            for image, target in eval_loader:\n",
    "                image = image.type(torch.FloatTensor).cuda()\n",
    "                logit = net(image)\n",
    "                y_pred+=list(logit.cpu().max(1)[1])\n",
    "                y_true+=list(target)\n",
    "                acc += logit.cpu().max(1)[1].eq(target).sum().numpy()\n",
    "\n",
    "                if n == 0:\n",
    "                        pred=logit\n",
    "                        correct=target\n",
    "                        n+=1\n",
    "                else:\n",
    "                    pred=torch.cat((pred, logit), axis=0)\n",
    "                    correct=torch.cat((correct, target), axis=0)\n",
    "        test_results_acc_seed_upsample.append(acc / len(test_dset_basic))\n",
    "        test_results_mcc_seed_upsample.append(mcc(pred, correct))\n",
    "    \n",
    "    upsampling_values=[\"up_\" + str(upsampling_value) for upsampling_value in upsampling_values]\n",
    "    test_results_acc_seed.append(dict(zip(upsampling_values, test_results_acc_seed_upsample)))\n",
    "    test_results_mcc_seed.append(dict(zip(upsampling_values, test_results_mcc_seed_upsample)))\n",
    "    \n",
    "seeds_sorted_idx=sorted(range(len(seeds)),key=seeds.__getitem__)\n",
    "seeds_sorted=[\"seed_\"+str(seeds[n]) for n in seeds_sorted_idx]\n",
    "test_results_acc_seed_sorted=[test_results_acc_seed[n] for n in seeds_sorted_idx]\n",
    "test_results_mcc_seed_sorted=[test_results_mcc_seed[n] for n in seeds_sorted_idx]\n",
    "    \n",
    "tests_results_acc_dict=dict(zip(seeds_sorted, test_results_acc_seed_sorted))\n",
    "tests_results_mcc_dict=dict(zip(seeds_sorted, test_results_mcc_seed_sorted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df=pd.DataFrame.from_dict(tests_results_acc_dict, orient='index',columns=upsampling_values)\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy mean over seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_df=pd.DataFrame.from_dict(tests_results_mcc_dict, orient='index',columns=upsampling_values)\n",
    "mcc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCC mean over seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end2end",
   "language": "python",
   "name": "end2end"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. - Settings and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export CUDA_VISIBLE_DEVICES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#Basic Imports\n",
    "\n",
    "os.chdir(\"..\")\n",
    "os\n",
    "import sys\n",
    "sys.path.insert(1, \"..\")\n",
    "from utils import decode_parameters_from_path\n",
    "\n",
    "from tqdm import tqdm,trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets.ssl_dataset import SSL_Dataset\n",
    "from datasets.data_utils import get_data_loader\n",
    "from utils import get_model_checkpoints\n",
    "from utils import net_builder\n",
    "import random\n",
    "from utils import clean_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary vs class names dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_dict={'eurosat_rgb' : ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway',\n",
    "       'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River',\n",
    "       'SeaLake'], 'eurosat_ms' : ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway',\n",
    "       'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River',\n",
    "       'SeaLake'], 'ucm' : [\"agricultural\", \"airplane\", \"baseballdiamond\", \"beach\", \"buildings\",\"chaparral\",\"denseresidential\",\"forest\", \"freeway\", \"golfcourse\",\"harbor\", \"intersection\", \"mediumresidential\", \"mobilehomepark\",\"overpass\",\"parkinglot\",\"river\", \"runway\", \"sparseresidential\", \"storagetanks\", \"tenniscourt\"],\n",
    "                 'thraws_swir' : ['event', 'notevent']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/home/gabrielemeoni/project/end2end/END2END/MSMatch/checkpoints/iter1/thraws_swir/FixMatch_archefficientnet-b0_batch16_confidence0.95_lr0.03_uratio4_wd0.00075_wu1.0_seed1_numlabels600_optSGD\" #Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folder for exported CSV files containing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder=\".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. - Parse checkpoint file and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "args = decode_parameters_from_path(os.path.join(checkpoint_path,\"\"))\n",
    "print(\"------------ RUNNING \", checkpoint_path, \" -----------------\")\n",
    "print(args)\n",
    "args[\"batch_size\"] = 256\n",
    "args[\"data_dir\"] = \"./data/\"\n",
    "args[\"use_train_model\"] = False\n",
    "args[\"load_path\"] = checkpoint_path\n",
    "\n",
    "checkpoint_model_path = os.path.join(checkpoint_path, \"model_best.pth\")\n",
    "if torch.cuda.is_available():\n",
    "    checkpoint = torch.load(checkpoint_model_path,map_location='cuda:0')\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint_model_path,map_location='cpu')\n",
    "    \n",
    "load_model = (checkpoint[\"train_model\"] if args[\"use_train_model\"] else checkpoint[\"eval_model\"])\n",
    "_net_builder = net_builder(args[\"net\"],False,{})\n",
    "_eval_dset = SSL_Dataset(name=args[\"dataset\"], train=False, data_dir=args[\"data_dir\"], seed=args[\"seed\"])\n",
    "eval_dset = _eval_dset.get_dset()\n",
    "net = _net_builder(num_classes=_eval_dset.num_classes, in_channels=_eval_dset.num_channels)\n",
    "net.load_state_dict(load_model)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "net.eval()\n",
    "\n",
    "eval_loader = get_data_loader(eval_dset, args[\"batch_size\"], num_workers=1)\n",
    "label_encoding = _eval_dset.label_encoding\n",
    "inv_transf = _eval_dset.inv_transform\n",
    "\n",
    "\n",
    "print(\"------------ PREDICTING TESTSET -----------------\")\n",
    "\n",
    "images, labels, preds = [],[],[]\n",
    "with torch.no_grad():\n",
    "    for image, target in tqdm(eval_loader):\n",
    "        image = image.type(torch.FloatTensor).cuda()\n",
    "        logit = net(image)\n",
    "        for idx,img in enumerate(image):\n",
    "            images.append(inv_transf(img.transpose(0,2).cpu().numpy()).transpose(0,2).numpy())\n",
    "        preds.append(logit.cpu().max(1)[1])\n",
    "        labels.append(target)\n",
    "labels = torch.cat(labels).numpy()\n",
    "preds = torch.cat(preds).numpy()\n",
    "test_report = classification_report(labels, preds, target_names=label_encoding, output_dict=True)\n",
    "test_report[\"params\"] = args\n",
    "results.append(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.DataFrame()\n",
    "pd.set_option('display.max_columns', None)\n",
    "for result in results:\n",
    "    params = result[\"params\"]\n",
    "    df = pd.DataFrame(result)\n",
    "    df.drop(list(params.keys()),inplace=True)\n",
    "    df.drop([\"support\",\"recall\",\"precision\"],inplace=True)\n",
    "    for key,val in params.items():\n",
    "        df[key] = val\n",
    "    df = df.set_index(\"dataset\")\n",
    "    big_df = big_df.append(df)\n",
    "# print(big_df)\n",
    "small_df = clean_results_df(big_df, \".\",\"numlabels\", keep_per_class=True)\n",
    "small_df.to_csv(csv_folder + \"_test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = small_df.drop(labels=[\"pretrained\",\"supervised\",\"net\",\"accuracy\",\"batch\",\"confidence\",\"lr\",\"uratio\",\"wd\",\"wu\",\"opt\",\"iterations\",\"load_path\"],axis=1)\n",
    "small_df = small_df.groupby('numlabels').mean().reset_index()\n",
    "small_df = small_df.reindex(sorted(small_df.columns), axis=1)\n",
    "small_df = small_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding info on numlabels per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(small_df[\"numlabels\"])):\n",
    "    small_df[\"numlabels\"][n]=str(small_df[\"numlabels\"][n]) + \" (\" + str(small_df[\"numlabels\"][n]//len(class_names_dict[args['dataset']]))+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = pd.melt(small_df, id_vars='numlabels', value_vars=class_names_dict[args['dataset']])\n",
    "l.columns = [\"# of labels \\n(per class)\", \"Class\", \"F1 Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. - Visualize results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize accuracy on subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set()\n",
    "sns.set(font_scale=3)  # crazy big\n",
    "with sns.plotting_context():\n",
    "    p = sns.catplot(x=\"F1 Score\", y=\"Class\", hue=\"# of labels \\n(per class)\", data=l, kind=\"bar\",palette=\"crest\",height=10,aspect=1.25)\n",
    "    # p.set_xticklabels(rotation=90)\n",
    "    p.set(xlim=[0.3,1.01])\n",
    "\n",
    "    # p.set(xticks=[0.4,0.6,0.8,1.0])\n",
    "plt.savefig(\"class_f1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc_dict={0:\"E\", 1: \"NE\"}\n",
    "idxs=random.sample([n for n in range(len(preds))], 9)\n",
    "images_to_plot=[]\n",
    "preds_to_plot=[]\n",
    "labels_to_plot=[]\n",
    "\n",
    "\n",
    "for idx in idxs:\n",
    "    images_to_plot.append(images[idx])\n",
    "    preds_to_plot.append(preds[idx])\n",
    "    labels_to_plot.append(labels[idx])\n",
    "    \n",
    "fig, ax=plt.subplots(3,3)\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "k=0\n",
    "for n in range(3):\n",
    "    for m in range(3):\n",
    "        ax[n,m].imshow(images_to_plot[k])\n",
    "        ax[n,m].set_title(\"GT:\"+str(label_enc_dict[labels_to_plot[k]])+\"\\nPR:\"+str(label_enc_dict[preds_to_plot[k]]), fontsize=10)\n",
    "        ax[n,m].imshow(images_to_plot[k])\n",
    "        ax[n,m].axis('off')\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end2end",
   "language": "python",
   "name": "end2end"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
